{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargar los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Miguel Alejandro\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import yaml\n",
    "\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ruta al archivo dataset.yaml\n",
    "yaml_path = \"path/to/dataset.yaml\"\n",
    "\n",
    "# Clases que quieres conservar (según el índice en los archivos .txt de etiquetas)\n",
    "clases_deseadas = {1, 3}  # Modifica según tus necesidades\n",
    "\n",
    "# Cargar el archivo YAML\n",
    "with open(yaml_path, \"r\") as f:\n",
    "    data = yaml.safe_load(f)\n",
    "\n",
    "# Obtener directorios desde el YAML\n",
    "image_dirs = [data[\"train\"], data.get(\"val\", \"\"), data.get(\"test\", \"\")]# cambiar\n",
    "label_dirs = [d.replace(\"images\", \"labels\") for d in image_dirs if d] # cambiar\n",
    "\n",
    "def tiene_clase_deseada(label_file, clases_deseadas):\n",
    "    \"\"\"Verifica si el archivo de etiqueta contiene alguna de las clases deseadas.\"\"\"\n",
    "    with open(label_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            class_id = int(line.split()[0])  # Primer número es la clase\n",
    "            if class_id in clases_deseadas:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "# Procesar cada conjunto (train, val, test si existen)\n",
    "for image_dir, label_dir in zip(image_dirs, label_dirs):\n",
    "    if not os.path.exists(label_dir):\n",
    "        continue  # Saltar si no existe la carpeta de etiquetas\n",
    "\n",
    "    for label_file in os.listdir(label_dir):\n",
    "        label_path = os.path.join(label_dir, label_file)\n",
    "        image_path = os.path.join(image_dir, label_file.replace(\".txt\", \".jpg\"))  # Ajusta si las imágenes son .png\n",
    "\n",
    "        if not tiene_clase_deseada(label_path, clases_deseadas):\n",
    "            os.remove(label_path)  # Eliminar etiqueta\n",
    "            if os.path.exists(image_path):\n",
    "                os.remove(image_path)  # Eliminar imagen\n",
    "\n",
    "print(\"Filtrado completado.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crear conjuntos de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sdadsfSSSSSSSSSSSSSSSSSSSSSSSSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Miguel Alejandro\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funciones útiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_iou(box1, box2):\n",
    "    \"\"\"Calcula el Intersection over Union (IoU) entre dos cajas [x_min, y_min, x_max, y_max].\"\"\"\n",
    "    x1_inter = max(box1[0], box2[0])\n",
    "    y1_inter = max(box1[1], box2[1])\n",
    "    x2_inter = min(box1[2], box2[2])\n",
    "    y2_inter = min(box1[3], box2[3])\n",
    "\n",
    "    # Calcular el área de intersección\n",
    "    inter_area = max(0, x2_inter - x1_inter) * max(0, y2_inter - y1_inter)\n",
    "\n",
    "    # Calcular el área de ambas cajas\n",
    "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "\n",
    "    # Calcular el IoU\n",
    "    iou = inter_area / float(box1_area + box2_area - inter_area) if (box1_area + box2_area - inter_area) > 0 else 0\n",
    "    return iou\n",
    "\n",
    "# def calcular_metricas_con_iou(predicciones, ground_truth, iou_threshold=0.5):\n",
    "#     \"\"\"Calcula precisión, recall y F1-score considerando IoU.\"\"\"\n",
    "#     resultado = {\"precision\": 0, \"recall\": 0, \"f1_score\": 0, \"class_comparison\": {}}\n",
    "\n",
    "#     clases = set(predicciones.keys()) | set(ground_truth.keys())\n",
    "#     total_tp, total_fp, total_fn = 0, 0, 0\n",
    "\n",
    "#     for clase in clases:\n",
    "#         pred_boxes = predicciones.get(clase, [])\n",
    "#         gt_boxes = ground_truth.get(clase, [])\n",
    "#         tp = 0\n",
    "#         matched_gt = set()\n",
    "\n",
    "#         for pred_box in pred_boxes:\n",
    "#             mejor_iou = 0\n",
    "#             mejor_gt_index = -1\n",
    "\n",
    "#             # Comparar con cada caja de la verdad\n",
    "#             for i, gt_box in enumerate(gt_boxes):\n",
    "#                 iou = calcular_iou(pred_box, gt_box)\n",
    "#                 if iou > mejor_iou and iou >= iou_threshold and i not in matched_gt:\n",
    "#                     mejor_iou = iou\n",
    "#                     mejor_gt_index = i\n",
    "\n",
    "#             if mejor_gt_index != -1:\n",
    "#                 tp += 1\n",
    "#                 matched_gt.add(mejor_gt_index)\n",
    "\n",
    "#         fp = len(pred_boxes) - tp  # Falsos Positivos: detecciones incorrectas\n",
    "#         fn = len(gt_boxes) - tp  # Falsos Negativos: detecciones faltantes\n",
    "\n",
    "#         total_tp += tp\n",
    "#         total_fp += fp\n",
    "#         total_fn += fn\n",
    "\n",
    "#         resultado[\"class_comparison\"][clase] = {\"tp\": tp, \"fp\": fp, \"fn\": fn}\n",
    "\n",
    "#     # Cálculo de precisión, recall y F1-score\n",
    "#     precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0\n",
    "#     recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0\n",
    "#     f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "#     resultado[\"precision\"] = precision\n",
    "#     resultado[\"recall\"] = recall\n",
    "#     resultado[\"f1_score\"] = f1\n",
    "\n",
    "#     return resultado\n",
    "\n",
    "def contar_detecciones_por_clase(data, selected_classes):\n",
    "    class_counts = defaultdict(int)\n",
    "    \n",
    "    for detections in data.values():  # Itera directamente sobre los valores (listas de detecciones)\n",
    "        for detection in detections:\n",
    "            class_name = detection['class']\n",
    "            if class_name in selected_classes:  # Filtrar solo las clases de interés\n",
    "                class_counts[class_name] += 1\n",
    "    \n",
    "    return dict(class_counts)\n",
    "\n",
    "def calcular_metricas_con_iou(predicciones, ground_truth, selected_classes=None, iou_threshold=0.3):\n",
    "    \"\"\"Calcula precisión, recall y F1-score considerando IoU para las clases seleccionadas.\"\"\"\n",
    "    resultado = {\"precision\": 0, \"recall\": 0, \"f1_score\": 0}\n",
    "\n",
    "    # Si no se especifican clases, usar todas las disponibles\n",
    "    if selected_classes is None:\n",
    "        selected_classes = set(predicciones.keys()) | set(ground_truth.keys())\n",
    "\n",
    "    total_tp, total_fp, total_fn = 0, 0, 0\n",
    "\n",
    "    for clase in selected_classes:\n",
    "        pred_boxes = predicciones.get(clase, [])\n",
    "        gt_boxes = ground_truth.get(clase, [])\n",
    "        tp = 0\n",
    "        matched_gt = set()\n",
    "\n",
    "        for pred_box in pred_boxes:\n",
    "            mejor_iou = 0\n",
    "            mejor_gt_index = -1\n",
    "\n",
    "            for i, gt_box in enumerate(gt_boxes):\n",
    "                iou = calcular_iou(pred_box[\"bbox\"], gt_box[\"bbox\"])\n",
    "                if iou > mejor_iou and iou >= iou_threshold and i not in matched_gt:\n",
    "                    mejor_iou = iou\n",
    "                    mejor_gt_index = i\n",
    "\n",
    "            if mejor_gt_index != -1:\n",
    "                tp += 1\n",
    "                matched_gt.add(mejor_gt_index)\n",
    "\n",
    "        fp = len(pred_boxes) - tp  # Falsos positivos\n",
    "        fn = len(gt_boxes) - tp  # Falsos negativos\n",
    "\n",
    "        total_tp += tp\n",
    "        total_fp += fp\n",
    "        total_fn += fn\n",
    "\n",
    "\n",
    "    # Calcular precisión y recall\n",
    "    precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0\n",
    "    recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0\n",
    "\n",
    "    # Calcular F1-score\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    resultado[\"precision\"] = precision\n",
    "    resultado[\"recall\"] = recall\n",
    "    resultado[\"f1_score\"] = f1\n",
    "\n",
    "    return resultado\n",
    "# Crear DataFrames para mostrar en tabla\n",
    "def generar_dataframe(data):\n",
    "    df_data = []\n",
    "    for modelo, metricas in data.items():\n",
    "        for idx, metrica in enumerate(metricas):\n",
    "            df_data.append({\"modelo\": modelo, \"iteración\": idx + 1, **metrica})\n",
    "    return pd.DataFrame(df_data) \n",
    "\n",
    "\n",
    "def calcular_metricas(conteo_modelo, conteo_gt):\n",
    "    \"\"\"Calcula precisión y recall basados en los conteos de clases.\"\"\"\n",
    "    clases = set(conteo_modelo.keys()) | set(conteo_gt.keys())\n",
    "    resultado = {\"precision\": 0, \"recall\": 0, \"f1_score\": {}}\n",
    "\n",
    "    total_tp, total_fp, total_fn = 0, 0, 0\n",
    "\n",
    "    for clase in clases:\n",
    "        tp = min(conteo_modelo.get(clase, 0), conteo_gt.get(clase, 0))  # Verdaderos Positivos: lo que coincida\n",
    "        fp = max(0, conteo_modelo.get(clase, 0) - tp)  # Falsos Positivos: detecciones extra\n",
    "        fn = max(0, conteo_gt.get(clase, 0) - tp)  # Falsos Negativos: detecciones faltantes\n",
    "\n",
    "        total_tp += tp\n",
    "        total_fp += fp\n",
    "        total_fn += fn\n",
    "\n",
    "        # resultado[\"class_comparison\"][clase] = {\"tp\": tp, \"fp\": fp, \"fn\": fn}\n",
    " \n",
    "    # Calcular presición y recall\n",
    "    precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0\n",
    "    recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0\n",
    "    # Calcular F1-score\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    resultado[\"precision\"] = precision\n",
    "    resultado[\"recall\"] = recall\n",
    "    # print(\"Precision: \", precision)\n",
    "    # print(\"Recall: \", recall)\n",
    "    resultado['f1_score'] = f1\n",
    "    print(\"F1-score: \", f1)\n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los archivos JSON\n",
    "models = [\"YOLO\",\"CBR1\",\"CBR2\",\"CBR3\",\"CBR4\"]\n",
    "paths = [\"./Results/CBR1\",\"./Results/CBR2\",\"./Results/CBR3\",\"./Results/CBR4\",\"./Results/YOLO\"]\n",
    "results = [\"/results1.json\",\"/results2.json\",\"/results3.json\",\"/results4.json\",\"/results5.json\"]\n",
    "path_gt = \"./Results/Truth\"\n",
    "\n",
    "data = {}\n",
    "for i in range(len(models)):\n",
    "    data[models[i]] = []\n",
    "    for elm in results:\n",
    "        with open(paths[i]+elm) as f:\n",
    "            model_data = json.load(f)\n",
    "            data[models[i]].append(model_data)\n",
    "ground_truth_data = []\n",
    "\n",
    "for i in range(len(results)):\n",
    "    with open(path_gt + results[i]) as f:\n",
    "        ground_truth_data.append(json.load(f))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score:  1.0\n",
      "F1-score:  1.0\n",
      "F1-score:  1.0\n",
      "F1-score:  1.0\n",
      "F1-score:  1.0\n",
      "F1-score:  1.0\n",
      "F1-score:  1.0\n",
      "F1-score:  1.0\n",
      "F1-score:  1.0\n",
      "F1-score:  1.0\n",
      "F1-score:  1.0\n",
      "F1-score:  1.0\n",
      "F1-score:  1.0\n",
      "F1-score:  1.0\n",
      "F1-score:  1.0\n",
      "F1-score:  1.0\n",
      "F1-score:  1.0\n",
      "F1-score:  1.0\n",
      "F1-score:  1.0\n",
      "F1-score:  1.0\n",
      "F1-score:  1.0\n",
      "F1-score:  1.0\n",
      "F1-score:  1.0\n",
      "F1-score:  1.0\n",
      "F1-score:  1.0\n"
     ]
    }
   ],
   "source": [
    "results  = {}\n",
    "correcto = [] \n",
    "for i in range(len(ground_truth_data)):  \n",
    "    conteo = contar_detecciones_por_clase(ground_truth_data[i], [\"Cladosporium\", \"Curvularia\"])\n",
    "    # print(conteo)\n",
    "    correcto.append(conteo)  \n",
    "        \n",
    "# Calcular métricas para cada modelo en \"data\"\n",
    "for model, lista in data.items():  \n",
    "    results[model] = []  \n",
    "\n",
    "    for i in range(len(lista)): \n",
    "        metricas = calcular_metricas(contar_detecciones_por_clase(lista[i], [\"Cladosporium\", \"Curvularia\"]), correcto[i])\n",
    "        results[model].append(metricas)\n",
    "\n",
    "\n",
    "# # Calcular métricas para cada modelo en \"data\"\n",
    "# for model, lista in data.items():  \n",
    "#     results[model] = []  \n",
    "\n",
    "#     for i in range(len(lista)):\n",
    "        \n",
    "#         metricas = calcular_metricas_con_iou(lista[i], ground_truth_data[i], [\"Cladosporium\", \"Curvularia\"])\n",
    "#         print(metricas)\n",
    "#         results[model].append(metricas)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tabla ANOVA:\n",
      "                    sum_sq    df         F    PR(>F)\n",
      "C(modelo)     4.901539e-31   4.0  0.263351  0.897161\n",
      "C(iteración)  7.836005e-32   4.0  0.042101  0.996280\n",
      "Residual      7.444875e-30  16.0       NaN       NaN\n",
      "\n",
      "Resultados del ANOVA (scipy.stats):\n",
      "F-statistic: nan\n",
      "p-value: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\scipy\\stats\\_stats_py.py:3895: ConstantInputWarning: Each of the input arrays is constant;the F statistic is not defined or infinite\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "df = generar_dataframe(results)\n",
    "# Organizar datos en formato ancho\n",
    "df_pivot = df.pivot(index='iteración', columns='modelo', values='f1_score').dropna()\n",
    "\n",
    "# Convertir a formato largo\n",
    "df_long = df_pivot.reset_index().melt(id_vars=['iteración'], var_name='modelo', value_name='f1_score')\n",
    "\n",
    "# ANOVA de medidas repetidas\n",
    "modelo_ols = ols('f1_score ~ C(modelo) + C(iteración)', data=df_long).fit()\n",
    "anova_table = sm.stats.anova_lm(modelo_ols, typ=2)\n",
    "print(\"\\nTabla ANOVA:\")\n",
    "print(anova_table)\n",
    "\n",
    "# Alternativamente, ANOVA con scipy.stats\n",
    "f1_score_groups = [df_pivot[modelo].values for modelo in df_pivot.columns]\n",
    "anova_result = stats.f_oneway(*f1_score_groups)\n",
    "print(\"\\nResultados del ANOVA (scipy.stats):\")\n",
    "print(\"F-statistic:\", anova_result.statistic)\n",
    "print(\"p-value:\", anova_result.pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prueba de normalidad (Shapiro-Wilk): ShapiroResult(statistic=1.0, pvalue=1.0)\n",
      "\n",
      "Prueba de homogeneidad de varianzas (Levene): LeveneResult(statistic=nan, pvalue=nan)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\scipy\\stats\\_morestats.py:1813: UserWarning: Input data for shapiro has range zero. The results may not be accurate.\n",
      "  warnings.warn(\"Input data for shapiro has range zero. The results \"\n",
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\scipy\\stats\\_morestats.py:2710: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  W = numer / denom\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Comprobar supuestos del ANOVA: Normalidad y Homogeneidad de varianzas\n",
    "# Prueba de normalidad (Shapiro-Wilk)\n",
    "shapiro_test = stats.shapiro(df_long['f1_score'])\n",
    "print(\"\\nPrueba de normalidad (Shapiro-Wilk):\", shapiro_test)\n",
    "\n",
    "# Prueba de homogeneidad de varianzas (Levene)\n",
    "levene_test = stats.levene(*[df_pivot[modelo].values for modelo in df_pivot.columns])\n",
    "print(\"\\nPrueba de homogeneidad de varianzas (Levene):\", levene_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resultados del Test de Friedman:\n",
      "Chi-cuadrado: nan\n",
      "p-value: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\scipy\\stats\\_stats_py.py:8696: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  chisq = (12.0 / (k*n*(k+1)) * ssbn - 3*n*(k+1)) / c\n"
     ]
    }
   ],
   "source": [
    "# # Si ANOVA no es válido, usar Test de Friedman\n",
    "# test_friedman = stats.friedmanchisquare(*[df_pivot[model].values for model in model_files])\n",
    "# print(\"\\nResultados del test de Friedman:\")\n",
    "# print(\"Chi-cuadrado:\", test_friedman.statistic)\n",
    "# print(\"p-value:\", test_friedman.pvalue)\n",
    "\n",
    "# Si ANOVA no es válido, usar Test de Friedman\n",
    "test_friedman = stats.friedmanchisquare(*[df_pivot[modelo].values for modelo in df_pivot.columns])\n",
    "print(\"\\nResultados del Test de Friedman:\")\n",
    "print(\"Chi-cuadrado:\", test_friedman.statistic)\n",
    "print(\"p-value:\", test_friedman.pvalue)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: scikit-posthocs\n",
      "Version: 0.11.2\n",
      "Summary: Statistical post-hoc analysis and outlier detection algorithms\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: Maksim Terpilovskii <maximtrp@gmail.com>\n",
      "License: Copyright (c) 2024 Maksim Terpilovskii\n",
      "\n",
      "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
      "of this software and associated documentation files (the \"Software\"), to deal\n",
      "in the Software without restriction, including without limitation the rights\n",
      "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
      "copies of the Software, and to permit persons to whom the Software is\n",
      "furnished to do so, subject to the following conditions:\n",
      "\n",
      "The above copyright notice and this permission notice shall be included in all\n",
      "copies or substantial portions of the Software.\n",
      "\n",
      "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
      "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
      "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
      "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
      "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
      "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
      "SOFTWARE.\n",
      "\n",
      "Location: C:\\Users\\Miguel Alejandro\\AppData\\Roaming\\Python\\Python311\\site-packages\n",
      "Requires: matplotlib, numpy, pandas, scipy, seaborn, statsmodels\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show scikit-posthocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import scikit_posthocs as sp\n",
    "# Si el Test de Friedman es significativo, realizar la prueba post-hoc de Nemenyi\n",
    "if test_friedman.pvalue < 0.05:\n",
    "    print(\"\\nPrueba post-hoc de Nemenyi:\")\n",
    "    nemenyi_results = sp.posthoc_nemenyi_friedman(df_pivot.values)\n",
    "    print(nemenyi_results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
